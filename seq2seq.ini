[strings]
# Mode : train, test
mode = train

train_enc = data/train.enc
train_dec = data/train.dec
test_enc = data/test.enc
test_dec = data/test.dec

train_vector_enc = data/train.enc.ids50000
train_vector_dec = data/train.dec.ids50000
test_vector_enc = data/test.enc.ids50000
test_vector_dec = data/test.dec.ids50000

resource_data = data/xiaohuangji50w_fenciA.conv

e = E
m = M

# folder where checkpoints, vocabulary, temporary data will be stored
working_directory = model
data_directory = data

[ints]
# vocabulary size 
enc_vocab_size = 50000
dec_vocab_size = 50000

# number of LSTM or GRU layers : 1/2/3
num_layers = 3

# typical options : 128, 256, 512, 1024
layer_size = 256

# dataset size
max_train_data_size = 50000
batch_size = 64

# steps per checkpoint
steps_per_checkpoint = 500

[floats]
learning_rate = 0.5
learning_rate_decay_factor = 0.99
max_gradient_norm = 5.0
